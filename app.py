import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import time
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import json
import os
from datetime import datetime, timedelta
import openai
from dotenv import load_dotenv
import traceback
import plotly.graph_objects as go
import schedule
import threading
import matplotlib.pyplot as plt

# ì›Œë“œí´ë¼ìš°ë“œ ì¶”ê°€
try:
    from wordcloud import WordCloud
except ImportError:
    st.error("wordcloud íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install wordcloud")
    WordCloud = None

# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í´ë˜ìŠ¤ ì¶”ê°€
class SchedulerState:
    def __init__(self):
        self.is_running = False
        self.thread = None
        self.last_run = None
        self.next_run = None
        self.scheduled_jobs = []
        self.scheduled_results = []

# ì „ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ê°ì²´ ìƒì„± (ìŠ¤ë ˆë“œ ì•ˆì—ì„œ ì‚¬ìš©)
global_scheduler_state = SchedulerState()

# API í‚¤ ê´€ë¦¬ë¥¼ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'openai_api_key' not in st.session_state:
    st.session_state.openai_api_key = None

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ ì‹œë„
load_dotenv()
if os.getenv('OPENAI_API_KEY'):
    st.session_state.openai_api_key = os.getenv('OPENAI_API_KEY')
elif 'OPENAI_API_KEY' in st.secrets:
    st.session_state.openai_api_key = st.secrets['OPENAI_API_KEY']

# í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')
    
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# OpenAI API í‚¤ ì„¤ì • (ì‹¤ì œ ì‚¬ìš© ì‹œ í™˜ê²½ ë³€ìˆ˜ë‚˜ Streamlit secretsì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)
if 'OPENAI_API_KEY' in os.environ:
    openai.api_key = os.environ['OPENAI_API_KEY']
elif 'OPENAI_API_KEY' in st.secrets:
    openai.api_key = st.secrets['OPENAI_API_KEY']
elif os.getenv('OPENAI_API_KEY'):
    openai.api_key = os.getenv('OPENAI_API_KEY')

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë‰´ìŠ¤ ê¸°ì‚¬ ë„êµ¬", page_icon="ğŸ“°", layout="wide")

# ì‚¬ì´ë“œë°” ë©”ë‰´ ì„¤ì •
st.sidebar.title("ë‰´ìŠ¤ ê¸°ì‚¬ ë„êµ¬")
menu = st.sidebar.radio(
    "ë©”ë‰´ ì„ íƒ",
    ["ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§", "ê¸°ì‚¬ ë¶„ì„í•˜ê¸°", "ìƒˆ ê¸°ì‚¬ ìƒì„±í•˜ê¸°", "ë‰´ìŠ¤ ê¸°ì‚¬ ì˜ˆì•½í•˜ê¸°"]
)

# ì €ì¥ëœ ê¸°ì‚¬ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
def load_saved_articles():
    if os.path.exists('saved_articles/articles.json'):
        with open('saved_articles/articles.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

# ê¸°ì‚¬ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_articles(articles):
    os.makedirs('saved_articles', exist_ok=True)
    with open('saved_articles/articles.json', 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

@st.cache_data
def crawl_naver_news(keyword, num_articles=5):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    """
    url = f"https://search.naver.com/search.naver?where=news&query={keyword}"
    results = []
    
    try:
        # í˜ì´ì§€ ìš”ì²­
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë‰´ìŠ¤ ì•„ì´í…œ ì°¾ê¸°
        news_items = soup.select('div.sds-comps-base-layout.sds-comps-full-layout')
        
        # ê° ë‰´ìŠ¤ ì•„ì´í…œì—ì„œ ì •ë³´ ì¶”ì¶œ
        for i, item in enumerate(news_items):
            if i >= num_articles:
                break
                
            try:
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                title_element = item.select_one('a.X0fMYp2dHd0TCUS2hjww span')
                if not title_element:
                    continue
                    
                title = title_element.text.strip()
                link_element = item.select_one('a.X0fMYp2dHd0TCUS2hjww')
                link = link_element['href'] if link_element else ""
                
                # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                press_element = item.select_one('div.sds-comps-profile-info-title span.sds-comps-text-type-body2')
                source = press_element.text.strip() if press_element else "ì•Œ ìˆ˜ ì—†ìŒ"
                
                # ë‚ ì§œ ì¶”ì¶œ
                date_element = item.select_one('span.r0VOr')
                date = date_element.text.strip() if date_element else "ì•Œ ìˆ˜ ì—†ìŒ"
                
                # ë¯¸ë¦¬ë³´ê¸° ë‚´ìš© ì¶”ì¶œ
                desc_element = item.select_one('a.X0fMYp2dHd0TCUS2hjww.IaKmSOGPdofdPwPE6cyU > span')
                description = desc_element.text.strip() if desc_element else "ë‚´ìš© ì—†ìŒ"
                
                results.append({
                    'title': title,
                    'link': link,
                    'description': description,
                    'source': source,
                    'date': date,
                    'content': ""  # ë‚˜ì¤‘ì— ì›ë¬¸ ë‚´ìš©ì„ ì €ì¥í•  í•„ë“œ
                })
                
            except Exception as e:
                st.error(f"ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                continue
                
    except Exception as e:
        st.error(f"í˜ì´ì§€ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
    return results

# ê¸°ì‚¬ ì›ë¬¸ ê°€ì ¸ì˜¤ê¸°
def get_article_content(url):
    try:
        response = requests.get(url, timeout=5)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì°¾ê¸°
        content = soup.select_one('#dic_area')
        if content:
            text = content.text.strip()
            text = re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°± ì œê±°
            return text
            
        # ë‹¤ë¥¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë³¸ë¬¸ ì°¾ê¸° (ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ëŒ€ì‘ í•„ìš”)
        content = soup.select_one('.article_body, .article-body, .article-content, .news-content-inner')
        if content:
            text = content.text.strip()
            text = re.sub(r'\s+', ' ', text)
            return text
            
        return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# NLTKë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ë¶„ì„
def analyze_keywords(text, top_n=10):
    # í•œêµ­ì–´ ë¶ˆìš©ì–´ ëª©ë¡ (ì§ì ‘ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤)
    korean_stopwords = ['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ë°', 'ë“±', 'ë¥¼', 'ì„', 'ì—', 'ì—ì„œ', 'ì˜', 'ìœ¼ë¡œ', 'ë¡œ']
    
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.isalnum() and len(word) > 1 and word not in korean_stopwords]
    
    word_count = Counter(tokens)
    top_keywords = word_count.most_common(top_n)
    
    return top_keywords

#ì›Œë“œ í´ë¼ìš°ë“œìš© ë¶„ì„
def extract_keywords_for_wordcloud(text, top_n=50):
    if not text or len(text.strip()) < 10:
        return {}

    try:
        try:
            tokens = word_tokenize(text.lower())
        except Exception as e:
            st.warning(f"{str(e)} ì˜¤ë¥˜ë°œìƒ")
            tokens = text.lower().split()
        
        stop_words = set()
        try:
            stop_words = set(stopwords.words('english'))
        except Exception:
            pass

        korea_stop_words = {
            'ë°', 'ë“±', 'ë¥¼', 'ì´', 'ì˜', 'ê°€', 'ì—', 'ëŠ”', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ê·¸', 'ë˜', 'ë˜ëŠ”', 'í•˜ëŠ”', 'í• ', 'í•˜ê³ ',
                'ìˆë‹¤', 'ì´ë‹¤', 'ìœ„í•´', 'ê²ƒì´ë‹¤', 'ê²ƒì€', 'ëŒ€í•œ', 'ë•Œë¬¸', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë˜ì„œ',
                'ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ìš”', 'ì£ ', 'ê³ ', 'ê³¼', 'ì™€', 'ë„', 'ì€', 'ìˆ˜', 'ê²ƒ', 'ë“¤', 'ì œ', 'ì €',
                'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ', 'ì§€ë‚œ', 'ì˜¬í•´', 'ë‚´ë…„', 'ìµœê·¼', 'í˜„ì¬', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ',
                'ì˜¤ì „', 'ì˜¤í›„', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'ê»˜ì„œ', 'ì´ë¼ê³ ', 'ë¼ê³ ', 'í•˜ë©°', 'í•˜ë©´ì„œ', 'ë”°ë¼', 'í†µí•´',
                'ê´€ë ¨', 'í•œí¸', 'íŠ¹íˆ', 'ê°€ì¥', 'ë§¤ìš°', 'ë”', 'ëœ', 'ë§ì´', 'ì¡°ê¸ˆ', 'í•­ìƒ', 'ìì£¼', 'ê°€ë”', 'ê±°ì˜',
                'ì „í˜€', 'ë°”ë¡œ', 'ì •ë§', 'ë§Œì•½', 'ë¹„ë¡¯í•œ', 'ë“±ì„', 'ë“±ì´', 'ë“±ì˜', 'ë“±ê³¼', 'ë“±ë„', 'ë“±ì—', 'ë“±ì—ì„œ',
                'ê¸°ì', 'ë‰´ìŠ¤', 'ì‚¬ì§„', 'ì—°í•©ë‰´ìŠ¤', 'ë‰´ì‹œìŠ¤', 'ì œê³µ', 'ë¬´ë‹¨', 'ì „ì¬', 'ì¬ë°°í¬', 'ê¸ˆì§€', 'ì•µì»¤', 'ë©˜íŠ¸',
                'ì¼ë³´', 'ë°ì¼ë¦¬', 'ê²½ì œ', 'ì‚¬íšŒ', 'ì •ì¹˜', 'ì„¸ê³„', 'ê³¼í•™', 'ì•„ì´í‹°', 'ë‹·ì»´', 'ì”¨ë„·', 'ë¸”ë¡œí„°', 'ì „ìì‹ ë¬¸'
        }
        stop_words.update(korea_stop_words)

        # 1ê¸€ì ì´ìƒì´ê³  ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ í† í°ë§Œ í•„í„°ë§
        filtered_tokens = [word for word in tokens if len(word) > 1 and word not in stop_words]
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_freq = {}
        for word in filtered_tokens:
            if word.isalnum():  # ì•ŒíŒŒë²³ê³¼ ìˆ«ìë§Œ í¬í•¨ëœ ë‹¨ì–´ë§Œ í—ˆìš©
                word_freq[word] = word_freq.get(word, 0) + 1
                
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ nê°œ ë°˜í™˜
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

        if not sorted_words:
            return {"data": 1, "analysis": 1, "news": 1}
        
        return dict(sorted_words[:top_n])
    
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ë°œìƒ {str(e)}")
        return {"data": 1, "analysis": 1, "news": 1}
    

# ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜

def generate_wordcloud(keywords_dict):
        if not WordCloud:
            st.warning("ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì¹˜ì•ˆë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return None
        try:
            wc= WordCloud(
                width=800,
                height=400,
                background_color = 'white',
                colormap = 'viridis',
                max_font_size=150,
                random_state=42
            ).generate_from_frequencies(keywords_dict)

            try:
                possible_font_paths=["NanumGothic.ttf", "ì´ë¦„"]

                font_path = None
                for path in possible_font_paths:
                    if os.path.exists(path):
                        font_path = path
                        break

                if font_path:
                    wc= WordCloud(
                        font_path=font_path,
                        width=800,
                        height=400,
                        background_color = 'white',
                        colormap = 'viridis',
                        max_font_size=150,
                        random_state=42
                    ).generate_from_frequencies(keywords_dict)
            except Exception as e:
                print(f"ì˜¤ë¥˜ë°œìƒ {str(e)}")
            
            return wc
        
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ë°œìƒ {str(e)}")
            return None

# ë‰´ìŠ¤ ë¶„ì„ í•¨ìˆ˜
def analyze_news_content(news_df):
    if news_df.empty:
        return "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
    
    results = {}
    #ì¹´í…Œê³ ë¦¬ë³„
    if 'source' in news_df.columns:
            results['source_counts'] = news_df['source'].value_counts().to_dict()
    #ì¹´í…Œê³ ë¦¬ë³„
    if 'date' in news_df.columns:
            results['date_counts'] = news_df['date'].value_counts().to_dict()

    #í‚¤ì›Œë“œë¶„ì„
    all_text = " ".join(news_df['title'].fillna('') + " " + news_df['content'].fillna(''))

    if len(all_text.strip()) > 0:
        results['top_keywords_for_wordcloud']= extract_keywords_for_wordcloud(all_text, top_n=50)
        results['top_keywords'] = analyze_keywords(all_text)
    else:
        results['top_keywords_for_wordcloud']={}
        results['top_keywords'] = []
    return results

# OpenAI APIë¥¼ ì´ìš©í•œ ìƒˆ ê¸°ì‚¬ ìƒì„±
def generate_article(original_content, prompt_text):
    try:
        response = openai.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë‰´ìŠ¤ ê¸°ìì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": f"ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {prompt_text}\n\n{original_content[:1000]}"}
            ],
            max_tokens=2000
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ê¸°ì‚¬ ìƒì„± ì˜¤ë¥˜: {str(e)}"

# OpenAI APIë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ìƒì„±
def generate_image(prompt):
    try:
        response = openai.images.generate(
            model="gpt-image-1",
            prompt=prompt
        )
        image_base64=response.data[0].b64_json
        return f"data:image/png;base64,{image_base64}"
    except Exception as e:
        return f"ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜: {str(e)}"

# ìŠ¤ì¼€ì¤„ëŸ¬ ê´€ë ¨ í•¨ìˆ˜ë“¤
def get_next_run_time(hour, minute):
    now = datetime.now()
    next_run = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
    if next_run <= now:
        next_run += timedelta(days=1)
    return next_run

def run_scheduled_task():
    try:
        while global_scheduler_state.is_running:
            schedule.run_pending()
            time.sleep(1)
    except Exception as e:
        print(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì—ëŸ¬ ë°œìƒ: {e}")
        traceback.print_exc()

def perform_news_task(task_type, keyword, num_articles, file_prefix):
    try:
        articles = crawl_naver_news(keyword, num_articles)
        
        # ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        for article in articles:
            article['content'] = get_article_content(article['link'])
            time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        # ê²°ê³¼ ì €ì¥
        os.makedirs('scheduled_news', exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"scheduled_news/{file_prefix}_{task_type}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        
        global_scheduler_state.last_run = datetime.now()
        print(f"{datetime.now()} - {task_type} ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ: {keyword}")
        
        # ì „ì—­ ìƒíƒœì— ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì €ì¥ (UI ì—…ë°ì´íŠ¸ìš©)
        result_item = {
            'task_type': task_type,
            'keyword': keyword,
            'timestamp': timestamp,
            'num_articles': len(articles),
            'filename': filename
        }
        global_scheduler_state.scheduled_results.append(result_item)
        
    except Exception as e:
        print(f"ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()

def start_scheduler(daily_tasks, interval_tasks):
    if not global_scheduler_state.is_running:
        schedule.clear()
        global_scheduler_state.scheduled_jobs = []
        
        # ì¼ë³„ íƒœìŠ¤í¬ ë“±ë¡
        for task in daily_tasks:
            hour = task['hour']
            minute = task['minute']
            keyword = task['keyword']
            num_articles = task['num_articles']
            
            job_id = f"daily_{keyword}_{hour}_{minute}"
            schedule.every().day.at(f"{hour:02d}:{minute:02d}").do(
                perform_news_task, "daily", keyword, num_articles, job_id
            ).tag(job_id)
            
            global_scheduler_state.scheduled_jobs.append({
                'id': job_id,
                'type': 'daily',
                'time': f"{hour:02d}:{minute:02d}",
                'keyword': keyword,
                'num_articles': num_articles
            })
        
        # ì‹œê°„ ê°„ê²© íƒœìŠ¤í¬ ë“±ë¡
        for task in interval_tasks:
            interval_minutes = task['interval_minutes']
            keyword = task['keyword']
            num_articles = task['num_articles']
            run_immediately = task['run_immediately']
            
            job_id = f"interval_{keyword}_{interval_minutes}"
            
            if run_immediately:
                # ì¦‰ì‹œ ì‹¤í–‰
                perform_news_task("interval", keyword, num_articles, job_id)
            
            # ë¶„ ê°„ê²©ìœ¼ë¡œ ì˜ˆì•½
            schedule.every(interval_minutes).minutes.do(
                perform_news_task, "interval", keyword, num_articles, job_id
            ).tag(job_id)
            
            global_scheduler_state.scheduled_jobs.append({
                'id': job_id,
                'type': 'interval',
                'interval': f"{interval_minutes}ë¶„ë§ˆë‹¤",
                'keyword': keyword,
                'num_articles': num_articles,
                'run_immediately': run_immediately
            })
        
        # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        next_run = schedule.next_run()
        if next_run:
            global_scheduler_state.next_run = next_run
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì“°ë ˆë“œ ì‹œì‘
        global_scheduler_state.is_running = True
        global_scheduler_state.thread = threading.Thread(
            target=run_scheduled_task, daemon=True
        )
        global_scheduler_state.thread.start()
        
        # ìƒíƒœë¥¼ ì„¸ì…˜ ìƒíƒœë¡œë„ ë³µì‚¬ (UI í‘œì‹œìš©)
        if 'scheduler_status' not in st.session_state:
            st.session_state.scheduler_status = {}
        
        st.session_state.scheduler_status = {
            'is_running': global_scheduler_state.is_running,
            'last_run': global_scheduler_state.last_run,
            'next_run': global_scheduler_state.next_run,
            'jobs_count': len(global_scheduler_state.scheduled_jobs)
        }

def stop_scheduler():
    if global_scheduler_state.is_running:
        global_scheduler_state.is_running = False
        schedule.clear()
        if global_scheduler_state.thread:
            global_scheduler_state.thread.join(timeout=1)
        global_scheduler_state.next_run = None
        global_scheduler_state.scheduled_jobs = []
        
        # UI ìƒíƒœ ì—…ë°ì´íŠ¸
        if 'scheduler_status' in st.session_state:
            st.session_state.scheduler_status['is_running'] = False

# ë©”ë‰´ì— ë”°ë¥¸ í™”ë©´ í‘œì‹œ
if menu == "ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§":
    st.header("ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§")
    
    keyword = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥", "ì¸ê³µì§€ëŠ¥")
    num_articles = st.slider("ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜", min_value=1, max_value=20, value=5)
    
    if st.button("ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°"):
        with st.spinner("ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤..."):
            articles = crawl_naver_news(keyword, num_articles)
            
            # ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            for i, article in enumerate(articles):
                st.progress((i + 1) / len(articles))
                article['content'] = get_article_content(article['link'])
                time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
            # ê²°ê³¼ ì €ì¥ ë° í‘œì‹œ
            save_articles(articles)
            st.success(f"{len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            
            # ìˆ˜ì§‘í•œ ê¸°ì‚¬ í‘œì‹œ
            for article in articles:
                with st.expander(f"{article['title']} - {article['source']}"):
                    st.write(f"**ì¶œì²˜:** {article['source']}")
                    st.write(f"**ë‚ ì§œ:** {article['date']}")
                    st.write(f"**ìš”ì•½:** {article['description']}")
                    st.write(f"**ë§í¬:** {article['link']}")
                    st.write("**ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:**")
                    st.write(article['content'][:300] + "...")

elif menu == "ê¸°ì‚¬ ë¶„ì„í•˜ê¸°":
    st.header("ê¸°ì‚¬ ë¶„ì„í•˜ê¸°")
    
    articles = load_saved_articles()
    if not articles:
        st.warning("ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§' ë©”ë‰´ì—ì„œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
    else:
        # ê¸°ì‚¬ ì„ íƒ
        titles = [article['title'] for article in articles]
        selected_title = st.selectbox("ë¶„ì„í•  ê¸°ì‚¬ ì„ íƒ", titles)
        
        selected_article = next((a for a in articles if a['title'] == selected_title), None)
        
        if selected_article:
            st.write(f"**ì œëª©:** {selected_article['title']}")
            st.write(f"**ì¶œì²˜:** {selected_article['source']}")
            
            # ë³¸ë¬¸ í‘œì‹œ
            with st.expander("ê¸°ì‚¬ ë³¸ë¬¸ ë³´ê¸°"):
                st.write(selected_article['content'])
            
            # ë¶„ì„ ë°©ë²• ì„ íƒ
            analysis_type = st.radio(
                "ë¶„ì„ ë°©ë²•",
                ["í‚¤ì›Œë“œ ë¶„ì„", "ê°ì • ë¶„ì„", "í…ìŠ¤íŠ¸ í†µê³„"]
            )
            
            if analysis_type == "í‚¤ì›Œë“œ ë¶„ì„":
                if st.button("í‚¤ì›Œë“œ ë¶„ì„í•˜ê¸°"):
                    with st.spinner("í‚¤ì›Œë“œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                        keyword_tab1, keyword_tab2 = st.tabs(["í‚¤ì›Œë“œ ë¹ˆë„", "ì›Œë“œí´ë¼ìš°ë“œ"])

                        with keyword_tab1:

                            keywords = analyze_keywords(selected_article['content'])
                            
                            # ì‹œê°í™”
                            df = pd.DataFrame(keywords, columns=['ë‹¨ì–´', 'ë¹ˆë„ìˆ˜'])
                            st.bar_chart(df.set_index('ë‹¨ì–´'))
                            
                            st.write("**ì£¼ìš” í‚¤ì›Œë“œ:**")
                            for word, count in keywords:
                                st.write(f"- {word}: {count}íšŒ")
                        with keyword_tab2:
                            keyword_dict = extract_keywords_for_wordcloud(selected_article['content'])
                            wc = generate_wordcloud(keyword_dict)
                            
                            if wc:
                                fig, ax = plt.subplots(figsize=(10, 5))
                                ax.imshow(wc, interpolation='bilinear')
                                ax.axis('off')
                                st.pyplot(fig)
                                
                                # í‚¤ì›Œë“œ ìƒìœ„ 20ê°œ í‘œì‹œ
                                st.write("**ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ:**")
                                top_keywords = sorted(keyword_dict.items(), key=lambda x: x[1], reverse=True)[:20]
                                keyword_df = pd.DataFrame(top_keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
                                st.dataframe(keyword_df)
                            else:
                                st.error("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            elif analysis_type == "í…ìŠ¤íŠ¸ í†µê³„":
                if st.button("í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„"):
                    content = selected_article['content']
                    
                    # í…ìŠ¤íŠ¸ í†µê³„ ê³„ì‚°
                    word_count = len(re.findall(r'\b\w+\b', content))
                    char_count = len(content)
                    sentence_count = len(re.split(r'[.!?]+', content))
                    avg_word_length = sum(len(word) for word in re.findall(r'\b\w+\b', content)) / word_count if word_count > 0 else 0
                    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0
                    
                    # í†µê³„ í‘œì‹œ
                    st.subheader("í…ìŠ¤íŠ¸ í†µê³„")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ë‹¨ì–´ ìˆ˜", f"{word_count:,}")
                    with col2:
                        st.metric("ë¬¸ì ìˆ˜", f"{char_count:,}")
                    with col3:
                        st.metric("ë¬¸ì¥ ìˆ˜", f"{sentence_count:,}")
                        
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", f"{avg_word_length:.1f}ì")
                    with col2:
                        st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{avg_sentence_length:.1f}ë‹¨ì–´")
                    
                    # í…ìŠ¤íŠ¸ ë³µì¡ì„± ì ìˆ˜ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
                    complexity_score = min(10, (avg_sentence_length / 10) * 5 + (avg_word_length / 5) * 5)
                    st.progress(complexity_score / 10)
                    st.write(f"í…ìŠ¤íŠ¸ ë³µì¡ì„± ì ìˆ˜: {complexity_score:.1f}/10")
                        
                    # ì¶œí˜„ ë¹ˆë„ ë§‰ëŒ€ ê·¸ë˜í”„
                    st.subheader("í’ˆì‚¬ë³„ ë¶„í¬ (í•œêµ­ì–´/ì˜ì–´ ì§€ì›)")
                    try:
                        # KoNLPy ì„¤ì¹˜ í™•ì¸
                        try:
                            from konlpy.tag import Okt
                            konlpy_installed = True
                        except ImportError:
                            konlpy_installed = False
                            st.warning("í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•´ KoNLPyë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install konlpy")
                        
                        # ì˜ì–´ POS tagger ì¤€ë¹„
                        from nltk import pos_tag
                        try:
                            nltk.data.find('taggers/averaged_perceptron_tagger')
                        except LookupError:
                            nltk.download('averaged_perceptron_tagger')
                        
                        # Try using the correct resource name as shown in the error message
                        try:
                            nltk.data.find('averaged_perceptron_tagger_eng')
                        except LookupError:
                            nltk.download('averaged_perceptron_tagger_eng')
                        
                        # ì–¸ì–´ ê°ì§€ (ê°„ë‹¨í•œ ë°©ì‹)
                        is_korean = bool(re.search(r'[ê°€-í£]', content))
                        
                        if is_korean and konlpy_installed:
                            # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„
                            okt = Okt()
                            tagged = okt.pos(content)
                            
                            # í•œêµ­ì–´ í’ˆì‚¬ ë§¤í•‘
                            pos_dict = {
                                'Noun': 'ëª…ì‚¬', 'NNG': 'ëª…ì‚¬', 'NNP': 'ê³ ìœ ëª…ì‚¬', 
                                'Verb': 'ë™ì‚¬', 'VV': 'ë™ì‚¬', 'VA': 'í˜•ìš©ì‚¬',
                                'Adjective': 'í˜•ìš©ì‚¬', 
                                'Adverb': 'ë¶€ì‚¬',
                                'Josa': 'ì¡°ì‚¬', 'Punctuation': 'êµ¬ë‘ì ',
                                'Determiner': 'ê´€í˜•ì‚¬', 'Exclamation': 'ê°íƒ„ì‚¬'
                            }
                            
                            pos_counts = {'ëª…ì‚¬': 0, 'ë™ì‚¬': 0, 'í˜•ìš©ì‚¬': 0, 'ë¶€ì‚¬': 0, 'ì¡°ì‚¬': 0, 'êµ¬ë‘ì ': 0, 'ê´€í˜•ì‚¬': 0, 'ê°íƒ„ì‚¬': 0, 'ê¸°íƒ€': 0}
                            
                            for _, pos in tagged:
                                if pos in pos_dict:
                                    pos_counts[pos_dict[pos]] += 1
                                elif pos.startswith('N'):  # ê¸°íƒ€ ëª…ì‚¬ë¥˜
                                    pos_counts['ëª…ì‚¬'] += 1
                                elif pos.startswith('V'):  # ê¸°íƒ€ ë™ì‚¬ë¥˜
                                    pos_counts['ë™ì‚¬'] += 1
                                else:
                                    pos_counts['ê¸°íƒ€'] += 1
                                    
                        else:
                            # ì˜ì–´ POS íƒœê¹…
                            tokens = word_tokenize(content.lower())
                            tagged = pos_tag(tokens)
                            
                            # ì˜ì–´ í’ˆì‚¬ ë§¤í•‘
                            pos_dict = {
                                'NN': 'ëª…ì‚¬', 'NNS': 'ëª…ì‚¬', 'NNP': 'ê³ ìœ ëª…ì‚¬', 'NNPS': 'ê³ ìœ ëª…ì‚¬', 
                                'VB': 'ë™ì‚¬', 'VBD': 'ë™ì‚¬', 'VBG': 'ë™ì‚¬', 'VBN': 'ë™ì‚¬', 'VBP': 'ë™ì‚¬', 'VBZ': 'ë™ì‚¬',
                                'JJ': 'í˜•ìš©ì‚¬', 'JJR': 'í˜•ìš©ì‚¬', 'JJS': 'í˜•ìš©ì‚¬',
                                'RB': 'ë¶€ì‚¬', 'RBR': 'ë¶€ì‚¬', 'RBS': 'ë¶€ì‚¬'
                            }
                            
                            pos_counts = {'ëª…ì‚¬': 0, 'ë™ì‚¬': 0, 'í˜•ìš©ì‚¬': 0, 'ë¶€ì‚¬': 0, 'ê¸°íƒ€': 0}
                            
                            for _, pos in tagged:
                                if pos in pos_dict:
                                    pos_counts[pos_dict[pos]] += 1
                                else:
                                    pos_counts['ê¸°íƒ€'] += 1
                        
                        # ê²°ê³¼ ì‹œê°í™”
                        pos_df = pd.DataFrame({
                            'í’ˆì‚¬': list(pos_counts.keys()),
                            'ë¹ˆë„': list(pos_counts.values())
                        })
                        
                        st.bar_chart(pos_df.set_index('í’ˆì‚¬'))
                        
                        if is_korean:
                            st.info("í•œêµ­ì–´ í…ìŠ¤íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        else:
                            st.info("ì˜ì–´ í…ìŠ¤íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"í’ˆì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                        st.error(traceback.format_exc())

            elif analysis_type == "ê°ì • ë¶„ì„":
                if st.button("ê°ì • ë¶„ì„í•˜ê¸°"):
                    if st.session_state.openai_api_key:
                        with st.spinner("ê¸°ì‚¬ì˜ ê°ì •ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                            try:
                                openai.api_key = st.session_state.openai_api_key
                                
                                # ê°ì • ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì„¤ì •
                                response = openai.chat.completions.create(
                                    model="gpt-4.1-mini",
                                    messages=[
                                        {"role": "system", "content": "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ì˜ ê°ì •ê³¼ ë…¼ì¡°ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì •ê³¼ ë…¼ì¡°ë¥¼ ë¶„ì„í•˜ê³ , 'ê¸ì •ì ', 'ë¶€ì •ì ', 'ì¤‘ë¦½ì ' ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ ì£¼ì„¸ìš”. ë˜í•œ ê¸°ì‚¬ì—ì„œ ë“œëŸ¬ë‚˜ëŠ” í•µì‹¬ ê°ì • í‚¤ì›Œë“œë¥¼ 5ê°œ ì¶”ì¶œí•˜ê³ , ê° í‚¤ì›Œë“œë³„ë¡œ 1-10 ì‚¬ì´ì˜ ê°•ë„ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•´ì£¼ì„¸ìš”: {'sentiment': 'ê¸ì •ì /ë¶€ì •ì /ì¤‘ë¦½ì ', 'reason': 'ì´ìœ  ì„¤ëª…...', 'keywords': [{'word': 'í‚¤ì›Œë“œ1', 'score': 8}, {'word': 'í‚¤ì›Œë“œ2', 'score': 7}, ...]}"},
                                        {"role": "user", "content": f"ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”:\n\nì œëª©: {selected_article['title']}\n\në‚´ìš©: {selected_article['content'][:1500]}"}
                                    ],
                                    max_tokens=800,
                                    response_format={"type": "json_object"}
                                )
                                
                                # JSON íŒŒì‹±
                                analysis_result = json.loads(response.choices[0].message.content)
                                
                                # ê²°ê³¼ ì‹œê°í™”
                                st.subheader("ê°ì • ë¶„ì„ ê²°ê³¼")
                                
                                # 1. ê°ì • íƒ€ì…ì— ë”°ë¥¸ ì‹œê°ì  í‘œí˜„
                                sentiment_type = analysis_result.get('sentiment', 'ì¤‘ë¦½ì ')
                                col1, col2, col3 = st.columns([1, 3, 1])
                                
                                with col2:
                                    if sentiment_type == "ê¸ì •ì ":
                                        st.markdown(f"""
                                        <div style="background-color:#DCEDC8; padding:20px; border-radius:10px; text-align:center;">
                                            <h1 style="color:#388E3C; font-size:28px;">ğŸ˜€ ê¸ì •ì  ë…¼ì¡° ğŸ˜€</h1>
                                            <p style="font-size:16px;">ê°ì • ê°•ë„: ë†’ìŒ</p>
                                        </div>
                                        """, unsafe_allow_html=True)
                                    elif sentiment_type == "ë¶€ì •ì ":
                                        st.markdown(f"""
                                        <div style="background-color:#FFCDD2; padding:20px; border-radius:10px; text-align:center;">
                                            <h1 style="color:#D32F2F; font-size:28px;">ğŸ˜ ë¶€ì •ì  ë…¼ì¡° ğŸ˜</h1>
                                            <p style="font-size:16px;">ê°ì • ê°•ë„: ë†’ìŒ</p>
                                        </div>
                                        """, unsafe_allow_html=True)
                                    else:
                                        st.markdown(f"""
                                        <div style="background-color:#E0E0E0; padding:20px; border-radius:10px; text-align:center;">
                                            <h1 style="color:#616161; font-size:28px;">ğŸ˜ ì¤‘ë¦½ì  ë…¼ì¡° ğŸ˜</h1>
                                            <p style="font-size:16px;">ê°ì • ê°•ë„: ì¤‘ê°„</p>
                                        </div>
                                        """, unsafe_allow_html=True)
                                
                                # 2. ì´ìœ  ì„¤ëª…
                                st.markdown("### ë¶„ì„ ê·¼ê±°")
                                st.markdown(f"<div style='background-color:#F5F5F5; padding:15px; border-radius:5px;'>{analysis_result.get('reason', '')}</div>", unsafe_allow_html=True)
                                
                                # 3. ê°ì • í‚¤ì›Œë“œ ì‹œê°í™”
                                st.markdown("### í•µì‹¬ ê°ì • í‚¤ì›Œë“œ")
                                
                                # í‚¤ì›Œë“œ ë°ì´í„° ì¤€ë¹„
                                keywords = analysis_result.get('keywords', [])
                                if keywords:
                                    # ë§‰ëŒ€ ì°¨íŠ¸ìš© ë°ì´í„°
                                    keyword_names = [item.get('word', '') for item in keywords]
                                    keyword_scores = [item.get('score', 0) for item in keywords]
                                    
                                    # ë ˆì´ë” ì°¨íŠ¸ ìƒì„±
                                    fig = go.Figure()
                                    
                                    # ìƒ‰ìƒ ì„¤ì •
                                    if sentiment_type == "ê¸ì •ì ":
                                        fill_color = 'rgba(76, 175, 80, 0.3)'  # ì—°í•œ ì´ˆë¡ìƒ‰
                                        line_color = 'rgba(76, 175, 80, 1)'     # ì§„í•œ ì´ˆë¡ìƒ‰
                                    elif sentiment_type == "ë¶€ì •ì ":
                                        fill_color = 'rgba(244, 67, 54, 0.3)'   # ì—°í•œ ë¹¨ê°„ìƒ‰
                                        line_color = 'rgba(244, 67, 54, 1)'     # ì§„í•œ ë¹¨ê°„ìƒ‰
                                    else:
                                        fill_color = 'rgba(158, 158, 158, 0.3)' # ì—°í•œ íšŒìƒ‰
                                        line_color = 'rgba(158, 158, 158, 1)'   # ì§„í•œ íšŒìƒ‰
                                    
                                    # ë ˆì´ë” ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„ - ë§ˆì§€ë§‰ ì ì´ ì²« ì ê³¼ ì—°ê²°ë˜ë„ë¡ ë°ì´í„° ì¶”ê°€
                                    radar_keywords = keyword_names.copy()
                                    radar_scores = keyword_scores.copy()
                                    
                                    # ë ˆì´ë” ì°¨íŠ¸ ìƒì„±
                                    fig.add_trace(go.Scatterpolar(
                                        r=radar_scores,
                                        theta=radar_keywords,
                                        fill='toself',
                                        fillcolor=fill_color,
                                        line=dict(color=line_color, width=2),
                                        name='ê°ì • í‚¤ì›Œë“œ'
                                    ))
                                    
                                    # ë ˆì´ë” ì°¨íŠ¸ ë ˆì´ì•„ì›ƒ ì„¤ì •
                                    fig.update_layout(
                                        polar=dict(
                                            radialaxis=dict(
                                                visible=True,
                                                range=[0, 10],
                                                tickmode='linear',
                                                tick0=0,
                                                dtick=2
                                            )
                                        ),
                                        showlegend=False,
                                        title={
                                            'text': 'ê°ì • í‚¤ì›Œë“œ ë ˆì´ë” ë¶„ì„',
                                            'y':0.95,
                                            'x':0.5,
                                            'xanchor': 'center',
                                            'yanchor': 'top'
                                        },
                                        height=500,
                                        width=500,
                                        margin=dict(l=80, r=80, t=80, b=80)
                                    )
                                    
                                    # ì°¨íŠ¸ ì¤‘ì•™ì— í‘œì‹œ
                                    col1, col2, col3 = st.columns([1, 2, 1])
                                    with col2:
                                        st.plotly_chart(fig)
                                    
                                    # í‚¤ì›Œë“œ ì¹´ë“œë¡œ í‘œì‹œ
                                    st.markdown("#### í‚¤ì›Œë“œ ì„¸ë¶€ ì„¤ëª…")
                                    cols = st.columns(min(len(keywords), 5))
                                    for i, keyword in enumerate(keywords):
                                        with cols[i % len(cols)]:
                                            word = keyword.get('word', '')
                                            score = keyword.get('score', 0)
                                            
                                            # ì ìˆ˜ì— ë”°ë¥¸ ìƒ‰ìƒ ê³„ì‚°
                                            r, g, b = 0, 0, 0
                                            if sentiment_type == "ê¸ì •ì ":
                                                g = min(200 + score * 5, 255)
                                                r = max(255 - score * 20, 100)
                                            elif sentiment_type == "ë¶€ì •ì ":
                                                r = min(200 + score * 5, 255)
                                                g = max(255 - score * 20, 100)
                                            else:
                                                r = g = b = 128
                                            
                                            # ì¹´ë“œ ìƒì„±
                                            st.markdown(f"""
                                            <div style="background-color:rgba({r},{g},{b},0.2); padding:10px; border-radius:5px; text-align:center; margin:5px;">
                                                <h3 style="margin:0;">{word}</h3>
                                                <div style="background-color:#E0E0E0; border-radius:3px; margin-top:5px;">
                                                    <div style="width:{score*10}%; background-color:rgba({r},{g},{b},0.8); height:10px; border-radius:3px;"></div>
                                                </div>
                                                <p style="margin:2px; font-size:12px;">ê°•ë„: {score}/10</p>
                                            </div>
                                            """, unsafe_allow_html=True)
                                    
                                else:
                                    st.info("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                                    
                                # 4. ìš”ì•½ í†µê³„
                                st.markdown("### ì£¼ìš” í†µê³„")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric(label="ê¸ì •/ë¶€ì • ì ìˆ˜", value=f"{7 if sentiment_type == 'ê¸ì •ì ' else 3 if sentiment_type == 'ë¶€ì •ì ' else 5}/10")
                                with col2:
                                    st.metric(label="í‚¤ì›Œë“œ ìˆ˜", value=len(keywords))
                                with col3:
                                    avg_score = sum(keyword_scores) / len(keyword_scores) if keyword_scores else 0
                                    st.metric(label="í‰ê·  ê°•ë„", value=f"{avg_score:.1f}/10")
                                
                            except Exception as e:
                                st.error(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                st.code(traceback.format_exc())
                    else:
                        st.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

elif menu == "ìƒˆ ê¸°ì‚¬ ìƒì„±í•˜ê¸°":
    st.header("ìƒˆ ê¸°ì‚¬ ìƒì„±í•˜ê¸°")
    
    articles = load_saved_articles()
    if not articles:
        st.warning("ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§' ë©”ë‰´ì—ì„œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
    else:
        # ê¸°ì‚¬ ì„ íƒ
        titles = [article['title'] for article in articles]
        selected_title = st.selectbox("ì›ë³¸ ê¸°ì‚¬ ì„ íƒ", titles)
        
        selected_article = next((a for a in articles if a['title'] == selected_title), None)
        
        if selected_article:
            st.write(f"**ì›ë³¸ ì œëª©:** {selected_article['title']}")
            
            with st.expander("ì›ë³¸ ê¸°ì‚¬ ë‚´ìš©"):
                st.write(selected_article['content'])
            
            prompt_text ="""ë‹¤ìŒ ê¸°ì‚¬ ì–‘ì‹ì„ ë”°ë¼ì„œ ë‹¤ì‹œ ì‘ì„±í•´ì¤˜. 
ì—­í• : ë‹¹ì‹ ì€ ì‹ ë¬¸ì‚¬ì˜ ê¸°ìì…ë‹ˆë‹¤.
ì‘ì—…: ìµœê·¼ ì¼ì–´ë‚œ ì‚¬ê±´ì— ëŒ€í•œ ë³´ë„ìë£Œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ìë£ŒëŠ” ì‚¬ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ê°ê´€ì ì´ê³  ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤.
ì§€ì¹¨:
ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ë¬¸ ë³´ë„ìë£Œ í˜•ì‹ì— ë§ì¶° ê¸°ì‚¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ê¸°ì‚¬ ì œëª©ì€ ì£¼ì œë¥¼ ëª…í™•íˆ ë°˜ì˜í•˜ê³  ë…ìì˜ ê´€ì‹¬ì„ ëŒ ìˆ˜ ìˆë„ë¡ ì‘ì„±í•©ë‹ˆë‹¤.
ê¸°ì‚¬ ë‚´ìš©ì€ ì •í™•í•˜ê³  ê°„ê²°í•˜ë©° ì„¤ë“ë ¥ ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
ê´€ë ¨ìì˜ ì¸í„°ë·°ë¥¼ ì¸ìš© í˜•íƒœë¡œ ë„£ì–´ì£¼ì„¸ìš”.
ìœ„ì˜ ì •ë³´ì™€ ì§€ì¹¨ì„ ì°¸ê³ í•˜ì—¬ ì‹ ë¬¸ ë³´ë„ìë£Œ í˜•ì‹ì˜ ê¸°ì‚¬ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”"""
            
            # ì´ë¯¸ì§€ ìƒì„± ì—¬ë¶€ ì„ íƒ ì˜µì…˜ ì¶”ê°€
            generate_image_too = st.checkbox("ê¸°ì‚¬ ìƒì„± í›„ ì´ë¯¸ì§€ë„ í•¨ê»˜ ìƒì„±í•˜ê¸°", value=True)
            
            if st.button("ìƒˆ ê¸°ì‚¬ ìƒì„±í•˜ê¸°"):
                if st.session_state.openai_api_key:
                    openai.api_key = st.session_state.openai_api_key
                    with st.spinner("ê¸°ì‚¬ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        new_article = generate_article(selected_article['content'], prompt_text)
                        
                        st.write("**ìƒì„±ëœ ê¸°ì‚¬:**")
                        st.write(new_article)
                        
                        # ì´ë¯¸ì§€ ìƒì„±í•˜ê¸° (ì˜µì…˜ì´ ì„ íƒëœ ê²½ìš°)
                        if generate_image_too:
                            with st.spinner("ê¸°ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                                # ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
                                image_prompt = f"""ì‹ ë¬¸ê¸°ì‚¬ ì œëª© "{selected_article['title']}" ì„ ë³´ê³  ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ì¤˜ 
                                ì´ë¯¸ì§€ì—ëŠ” ë‹¤ìŒ ìš”ì†Œê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:
                                - ê¸°ì‚¬ë¥¼ ì´í•´í•  ìˆ˜ ìˆëŠ” ë„ì‹
                                - ê¸°ì‚¬ ë‚´ìš©ê³¼ ê´€ë ¨ëœ í…ìŠ¤íŠ¸ 
                                - ì‹¬í”Œí•˜ê²Œ ì²˜ë¦¬
                                """
                                
                                # ì´ë¯¸ì§€ ìƒì„±
                                image_url = generate_image(image_prompt)
                                
                                if image_url and not image_url.startswith("ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜"):
                                    st.subheader("ìƒì„±ëœ ì´ë¯¸ì§€:")
                                    st.image(image_url)
                                else:
                                    st.error(image_url)
                        
                        # ìƒì„±ëœ ê¸°ì‚¬ ì €ì¥ ì˜µì…˜
                        if st.button("ìƒì„±ëœ ê¸°ì‚¬ ì €ì¥"):
                            new_article_data = {
                                'title': f"[ìƒì„±ë¨] {selected_article['title']}",
                                'source': f"AI ìƒì„± (ì›ë³¸: {selected_article['source']})",
                                'date': datetime.now().strftime("%Y-%m-%d %H:%M"),
                                'description': new_article[:100] + "...",
                                'link': "",
                                'content': new_article
                            }
                            articles.append(new_article_data)
                            save_articles(articles)
                            st.success("ìƒì„±ëœ ê¸°ì‚¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    st.warning("OpenAI API í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •í•´ì£¼ì„¸ìš”.")



elif menu == "ë‰´ìŠ¤ ê¸°ì‚¬ ì˜ˆì•½í•˜ê¸°":
    st.header("ë‰´ìŠ¤ ê¸°ì‚¬ ì˜ˆì•½í•˜ê¸°")
    
    # íƒ­ ìƒì„±
    tab1, tab2, tab3 = st.tabs(["ì¼ë³„ ì˜ˆì•½", "ì‹œê°„ ê°„ê²© ì˜ˆì•½", "ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ"])
    
    # ì¼ë³„ ì˜ˆì•½ íƒ­
    with tab1:
        st.subheader("ë§¤ì¼ ì •í•´ì§„ ì‹œê°„ì— ê¸°ì‚¬ ìˆ˜ì§‘í•˜ê¸°")
        
        # í‚¤ì›Œë“œ ì…ë ¥
        daily_keyword = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value="ì¸ê³µì§€ëŠ¥", key="daily_keyword")
        daily_num_articles = st.slider("ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜", min_value=1, max_value=20, value=5, key="daily_num_articles")
        
        # ì‹œê°„ ì„¤ì •
        daily_col1, daily_col2 = st.columns(2)
        with daily_col1:
            daily_hour = st.selectbox("ì‹œ", range(24), format_func=lambda x: f"{x:02d}ì‹œ", key="daily_hour")
        with daily_col2:
            daily_minute = st.selectbox("ë¶„", range(0, 60, 5), format_func=lambda x: f"{x:02d}ë¶„", key="daily_minute")
        
        # ì¼ë³„ ì˜ˆì•½ ë¦¬ìŠ¤íŠ¸
        if 'daily_tasks' not in st.session_state:
            st.session_state.daily_tasks = []
        
        if st.button("ì¼ë³„ ì˜ˆì•½ ì¶”ê°€"):
            st.session_state.daily_tasks.append({
                'hour': daily_hour,
                'minute': daily_minute,
                'keyword': daily_keyword,
                'num_articles': daily_num_articles
            })
            st.success(f"ì¼ë³„ ì˜ˆì•½ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤: ë§¤ì¼ {daily_hour:02d}:{daily_minute:02d} - '{daily_keyword}'")
        
        # ì˜ˆì•½ ëª©ë¡ í‘œì‹œ
        if st.session_state.daily_tasks:
            st.subheader("ì¼ë³„ ì˜ˆì•½ ëª©ë¡")
            for i, task in enumerate(st.session_state.daily_tasks):
                st.write(f"{i+1}. ë§¤ì¼ {task['hour']:02d}:{task['minute']:02d} - '{task['keyword']}' ({task['num_articles']}ê°œ)")
            
            if st.button("ì¼ë³„ ì˜ˆì•½ ì´ˆê¸°í™”"):
                st.session_state.daily_tasks = []
                st.warning("ì¼ë³„ ì˜ˆì•½ì´ ëª¨ë‘ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì‹œê°„ ê°„ê²© ì˜ˆì•½ íƒ­
    with tab2:
        st.subheader("ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘í•˜ê¸°")
        
        # í‚¤ì›Œë“œ ì…ë ¥
        interval_keyword = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value="ë¹…ë°ì´í„°", key="interval_keyword")
        interval_num_articles = st.slider("ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜", min_value=1, max_value=20, value=5, key="interval_num_articles")
        
        # ì‹œê°„ ê°„ê²© ì„¤ì •
        interval_minutes = st.number_input("ì‹¤í–‰ ê°„ê²©(ë¶„)", min_value=1, max_value=60*24, value=30, key="interval_minutes")
        
        # ì¦‰ì‹œ ì‹¤í–‰ ì—¬ë¶€
        run_immediately = st.checkbox("ì¦‰ì‹œ ì‹¤í–‰", value=True, help="ì²´í¬í•˜ë©´ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹œ ì¦‰ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        
        # ì‹œê°„ ê°„ê²© ì˜ˆì•½ ë¦¬ìŠ¤íŠ¸
        if 'interval_tasks' not in st.session_state:
            st.session_state.interval_tasks = []
        
        if st.button("ì‹œê°„ ê°„ê²© ì˜ˆì•½ ì¶”ê°€"):
            st.session_state.interval_tasks.append({
                'interval_minutes': interval_minutes,
                'keyword': interval_keyword,
                'num_articles': interval_num_articles,
                'run_immediately': run_immediately
            })
            st.success(f"ì‹œê°„ ê°„ê²© ì˜ˆì•½ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤: {interval_minutes}ë¶„ë§ˆë‹¤ - '{interval_keyword}'")
        
        # ì˜ˆì•½ ëª©ë¡ í‘œì‹œ
        if st.session_state.interval_tasks:
            st.subheader("ì‹œê°„ ê°„ê²© ì˜ˆì•½ ëª©ë¡")
            for i, task in enumerate(st.session_state.interval_tasks):
                immediate_text = "ì¦‰ì‹œ ì‹¤í–‰ í›„ " if task['run_immediately'] else ""
                st.write(f"{i+1}. {immediate_text}{task['interval_minutes']}ë¶„ë§ˆë‹¤ - '{task['keyword']}' ({task['num_articles']}ê°œ)")
            
            if st.button("ì‹œê°„ ê°„ê²© ì˜ˆì•½ ì´ˆê¸°í™”"):
                st.session_state.interval_tasks = []
                st.warning("ì‹œê°„ ê°„ê²© ì˜ˆì•½ì´ ëª¨ë‘ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ íƒ­
    with tab3:
        st.subheader("ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´ ë° ìƒíƒœ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘/ì¤‘ì§€ ë²„íŠ¼
            if not global_scheduler_state.is_running:
                if st.button("ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"):
                    if not st.session_state.daily_tasks and not st.session_state.interval_tasks:
                        st.error("ì˜ˆì•½ëœ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¼ë³„ ì˜ˆì•½ ë˜ëŠ” ì‹œê°„ ê°„ê²© ì˜ˆì•½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
                    else:
                        start_scheduler(st.session_state.daily_tasks, st.session_state.interval_tasks)
                        st.success("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                if st.button("ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"):
                    stop_scheduler()
                    st.warning("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        with col2:
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í‘œì‹œ
            if 'scheduler_status' in st.session_state:
                st.write(f"ìƒíƒœ: {'ì‹¤í–‰ì¤‘' if global_scheduler_state.is_running else 'ì¤‘ì§€'}")
                if global_scheduler_state.last_run:
                    st.write(f"ë§ˆì§€ë§‰ ì‹¤í–‰: {global_scheduler_state.last_run.strftime('%Y-%m-%d %H:%M:%S')}")
                if global_scheduler_state.next_run and global_scheduler_state.is_running:
                    st.write(f"ë‹¤ìŒ ì‹¤í–‰: {global_scheduler_state.next_run.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                st.write("ìƒíƒœ: ì¤‘ì§€")
        
        # ì˜ˆì•½ëœ ì‘ì—… ëª©ë¡
        if global_scheduler_state.scheduled_jobs:
            st.subheader("í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì˜ˆì•½ ì‘ì—…")
            for i, job in enumerate(global_scheduler_state.scheduled_jobs):
                if job['type'] == 'daily':
                    st.write(f"{i+1}. [ì¼ë³„] ë§¤ì¼ {job['time']} - '{job['keyword']}' ({job['num_articles']}ê°œ)")
                else:
                    immediate_text = "[ì¦‰ì‹œ ì‹¤í–‰ í›„] " if job.get('run_immediately', False) else ""
                    st.write(f"{i+1}. [ê°„ê²©] {immediate_text}{job['interval']} - '{job['keyword']}' ({job['num_articles']}ê°œ)")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ê²°ê³¼
        if global_scheduler_state.scheduled_results:
            st.subheader("ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ê²°ê³¼")
            
            # ê²°ê³¼ë¥¼ UIì— í‘œì‹œí•˜ê¸° ì „ì— ë³µì‚¬
            results_for_display = global_scheduler_state.scheduled_results.copy()
            
            if results_for_display:
                result_df = pd.DataFrame(results_for_display)
                result_df['ì‹¤í–‰ì‹œê°„'] = result_df['timestamp'].apply(lambda x: datetime.strptime(x, "%Y%m%d_%H%M%S").strftime("%Y-%m-%d %H:%M:%S"))
                result_df = result_df.rename(columns={
                    'task_type': 'ì‘ì—…ìœ í˜•', 
                    'keyword': 'í‚¤ì›Œë“œ', 
                    'num_articles': 'ê¸°ì‚¬ìˆ˜', 
                    'filename': 'íŒŒì¼ëª…'
                })
                result_df['ì‘ì—…ìœ í˜•'] = result_df['ì‘ì—…ìœ í˜•'].apply(lambda x: 'ì¼ë³„' if x == 'daily' else 'ì‹œê°„ê°„ê²©')
                
                st.dataframe(
                    result_df[['ì‘ì—…ìœ í˜•', 'í‚¤ì›Œë“œ', 'ê¸°ì‚¬ìˆ˜', 'ì‹¤í–‰ì‹œê°„', 'íŒŒì¼ëª…']],
                    hide_index=True
                )
        
        # ìˆ˜ì§‘ëœ íŒŒì¼ ë³´ê¸°
        if os.path.exists('scheduled_news'):
            files = [f for f in os.listdir('scheduled_news') if f.endswith('.json')]
            if files:
                st.subheader("ìˆ˜ì§‘ëœ íŒŒì¼ ì—´ê¸°")
                selected_file = st.selectbox("íŒŒì¼ ì„ íƒ", files, index=len(files)-1)
                if selected_file and st.button("íŒŒì¼ ë‚´ìš© ë³´ê¸°"):
                    with open(os.path.join('scheduled_news', selected_file), 'r', encoding='utf-8') as f:
                        articles = json.load(f)
                    
                    st.write(f"**íŒŒì¼ëª…:** {selected_file}")
                    st.write(f"**ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜:** {len(articles)}ê°œ")
                    
                    for article in articles:
                        with st.expander(f"{article['title']} - {article['source']}"):
                            st.write(f"**ì¶œì²˜:** {article['source']}")
                            st.write(f"**ë‚ ì§œ:** {article['date']}")
                            st.write(f"**ë§í¬:** {article['link']}")
                            st.write("**ë³¸ë¬¸:**")
                            st.write(article['content'][:500] + "..." if len(article['content']) > 500 else article['content'])

# í‘¸í„°
st.markdown("---")
st.markdown("Â© ë‰´ìŠ¤ ê¸°ì‚¬ ë„êµ¬ @conanssam")
